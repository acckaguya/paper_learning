# 强化学习

**简介**：强化学习介于无监督学习与监督学习之间，其强调智能体和环境的交互并采取行动。在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。在每个特定时间点，智能体从环境 接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将 其传输回环境，最后智能体从环境中获得奖励（reward）。此后新一轮循环开始，智能体接收后续观察，并 选择后续操作，依此类推。
![[Pasted image 20250723163236.png]]
## 1 马尔可夫决策过程（MDP）

MDP（Markov Decision Proces， 马尔可夫决策过程）。决策过程是指智能代理通过与环境交互决定其行动的过程。

相关概念：
- 状态（state）：智能代理所处的情况根据其的行动发生改变。状态取决于智能体的行动，智能体在状态迁移后采取新的行动。
- 动作（Action）：智能体根据所处的状态和环境做出的行动。
- 奖励（Reward）：智能体在执行相应动作后获得的奖励。

MDP时间序列数据：
![[Pasted image 20250723163944.png]]

MDP通过数学的方式来表示智能代理、环境及二者之间的互动，其中有以下三个要素：
- 状态迁移：状态如何迁移。
- 奖励：如何给予奖励。
- 策略：智能代理如何决定行动。

#### 状态迁移

在确定的状态迁移的情况下，下一个状态由当前状态和行动唯一决定，其表示为：
![[Pasted image 20250723164334.png]]

在随机性行动的情况下，状态迁移概率如下：
![[Pasted image 20250723164433.png]]

马尔可夫性：状态迁移不需要过去的信息，仅仅取决于当前状态以及执行了什么行动。

#### 奖励函数

当智能代理处于状态s，执行行动a，下一个状态为s'时，奖励函数(reward function)由**r(s, a, s')** 定义。

#### 智能代理的策略

智能代理的策略表示智能代理如何决定其的行动。智能代理的策略根据当前状态决定。智能代理的行动可以是确定的，也可以是随机的。

确定性策略定义为以下函数：
![[Pasted image 20250723165045.png]]

随机性策略定义为以下函数：
![[Pasted image 20250723165124.png]]

#### MDP的目标

智能代理根据策略采取行动。首先，它根据它的行 动和状态迁移概率迁移到下一个状态。然后，它根据奖励函数获得奖励。在这个框架内，**MDP的目标是找到最优策略（optimal policy）。最优策略是使收益最大化的策略。**

#### 收益

智能代理的目标就是使收益（Return）最大化。

设时刻为$t$，状态为$S_t$；（其中$t$是任意值）。然后，智能代理根据策略执行行动，获得奖励，之后迁移到新状态，这个过程不断重复进行。在这种情况下，收益$G_t$的定 义如下所示。
![[Pasted image 20250723170115.png]]

$\gamma$ 代表折现率，其的值被设于0.0到1.0之间的实数。引入折现率是为了防止连续性任务的收益变得无穷大。

#### 状态价值函数

为了处理随机行动，需要使用期望值作为衡量收益的标准。收益的期望如下：
![[Pasted image 20250723170447.png]]
$v_{\pi}(s)$ 表示状态价值函数（state-value function）。

最优策略的状态价值函数就叫做最有状态价值函数（optimal state-value function）。


## 2 贝尔曼方程

在环境不确定，MDP的行动是随机的情况下，状态价值函数无法直接通过数学计算直接得出，因此需要贝尔曼方程估计状态价值函数。

此处省略推导过程，直接给出贝尔曼方程：
![[Pasted image 20250723171000.png]]

贝尔曼方程可以将无限的计算转换为有限的联立方程。

#### 行动价值函数

在给出状态$s$和策略$\pi$，在次基础上增加一个额外的条件：行动$a$。 行动价值函数的数学表达式如下：
![[Pasted image 20250723171251.png]]

行动价值函数的意义为：在时刻$t$的状态$s$下采取行动$a$（采取该行动与策略无关，可以自由决定），并从时刻$t+1$开始根据策略$\pi$采取行动能得到收益的期望值。

**状态价值函数和行动价值函数的关系** ：
![[Pasted image 20250723171548.png]]
使用行动价值函数的贝尔曼函数：
![[Pasted image 20250723171631.png]]
![[Pasted image 20250723171638.png]]

#### “最优”
![[Pasted image 20250723171718.png]]

## 3 动态规划法

通过贝尔曼方程，我们可以通过链里多个方程求解出价值函数，但这种方法在状态和行动的数量很大时会难以使用。使用**动态规划法**可以对价值函数进行评估。

强化学习问题通常设计两项任务：**策略评估** 和 **策略控制** 。策略评估是求给定策略的状态价值函数和行动价值函数。策略控制是控制策略并将其调整为最优策略。


#### 迭代策略评估

通过动态规划思想，可以将贝尔曼方程变形为使用“自举法”的更新式：
![[Pasted image 20250723172547.png]]

首先为$V_0(s)$ 设置初始值(比如在所有状态下进行$V_0(s) = 0$的初始化操作)。然后利用式4.2。通过重复这个过程，我们就接近了最终的目标 $V_{\pi}(s)$。这种算法被称为迭代策略评估(iterative policy evaluation) 。

#### 策略迭代法

![[Pasted image 20250723173059.png]]

式子中各部分含义如下：
- 当前策略：$\mu(s)$
- 当前策略的状态价值函数：$v_{\mu}(s)$
- 新的策略：$\mu'(s)$
通过上式更新策略的方法称为”贪婪化“。

策略迭代法的流程如下：
![[Pasted image 20250723173413.png]]![[Pasted image 20250723173603.png]]

将该流程持续下去，最终会达到策略无法再通过贪婪化进行更新的情况。此时的策略就是最优策略。

#### 价值迭代法

策略迭代法是对策略进行更新，如果对价值进行更新，则引出价值迭代法。

策略迭代法中的贪婪化更新策略和评估价值函数中存在重复计算，将argmax算子消去后可以得到以下算式：
![[Pasted image 20250723174039.png]]

价值迭代法根据上式进行更新。

价值迭代法在经过无限次更新后会得到最优价值函数。但在实践中，我们需要在某一点停止更新。实现这个目标的一种方法是使用阈值。事先设定阈值，当所有状态的更新量低于阈值时停止更新。

得到$V_*(s)$后，最优策略$\mu*(s)$：
![[Pasted image 20250723174230.png]]


## 4 蒙特卡洛方法

使用DP方法得到最优价值函数和最优策略的前提是环境模型必须是已知的，也就是状态迁移概率和奖励函数必须是已知的。但在某些问题中，环境模型可能是未知的，在这种情况下，可以使用**蒙特卡洛方法**来估计价值函数。

蒙特卡洛方法是对数据进行反复采样并根据结果进行估计的方法的总称。在强化学习中，蒙特卡洛方法可以用来从经验中估计价值函数。这里的“经验”是指从环 境和智能代理之间的实际互动中获得的数据。

具体方法为**做大量的采样并取平均值**。

#### 蒙特卡洛方法计算价值函数

![[Pasted image 20250723174922.png]]
这里用G表示从状态s开始获得的收益，用$G^{(i)}$表示在第i回合获得的收益。为了用蒙特卡洛方法计算，我们要像式5.2所示那样运行n个回合， 并计算得到的样本数据的平均值。

注：蒙特卡洛方法只适用于回合制任务。


#### 评估和改进

在改进策略上依旧沿用贪婪化方法：
![[Pasted image 20250723180240.png]]

对状态价值函数和行动价值函数的评估：
![[Pasted image 20250723180310.png]]
#### 同策略型(on-policy)和异策略型(off-policy)

两种策略：
- 目标策略：作为评估和改进对象的策略，对这种策略进行评估和改进。
- 行为策略：智能代理会基于这种策略产生状态、行动和奖励的样本数据。

**异策略型**意味着分开考虑目标侧录和行为策略。

通过异策略型，我们可以只让行为策略进行探索，只让目标策略进行利用。但在使用从行为策略获得的样本数据来求目标策略的期望值时，我们需要一些计算上的技巧。此时用到的就是重要性采样(importance sampling)这项技术。

#### 重要性采样

重要性采样利用从其他概率分布中采样的数据来计算某个概率分布的期望值。

重要性采样推导：
![[Pasted image 20250723182237.png]]![[Pasted image 20250723182244.png]]

在该式子中的$E_b$表示概率分布b的期望值。基于上式的蒙特卡洛方法如下：
![[Pasted image 20250723182342.png]]

重要性采样方法会导致较大的方差。若两个概率分布差别过大，方差则增大。减小方差的一个方法就是使两个概率分布尽可能地接近，使得权重$\rho$ 的值更加接近1。  


## 5 TD方法

使用蒙特卡洛方法对策略进行评估时，只能在达到回合终点时更新价值函数。**TD方法（Temporal Difference， 时间差分）不适用环境模型，且每次采取行动时都会更新价值函数。**

价值函数定义：
![[Pasted image 20250723183044.png]]

蒙特卡洛方法可以由6.3导出，动态规划方法由6.4导出。
二者的比较如下：
![[Pasted image 20250723183237.png]]

将两种方法融合起来的做法就是TD方法：
![[Pasted image 20250723183313.png]]

TD方法的更新式如下：
![[Pasted image 20250723183512.png]]
算式中的$R_t+\gamma v_{\pi}(S_{t+1})$ 叫做**TD目标**， TD方法会朝着TD目标的方向更新$V_{\pi}(S_t)$。

#### 蒙特卡洛方法和TD方法的比较

![[Pasted image 20250723183737.png]]

蒙特卡洛方法是朝着达到目标后获得的收益的样本数据的方向更新价值函数，TD方法是根据下一步的信息来更新函数。

蒙特卡洛方法是积累了大量时间步数据得到的结果，因此偏差会比较大，TD方法则偏差较小。蒙特卡洛方法中不含估计值，因此其的目标是“无偏置的”，TD方法采用自举法来更新目标，因此是“有偏置的”。

#### 同策略型的SARSA

在进行策略控制时，我们需要使用行动价值函数作为评估对象。

使用行动价值函数评估时的更新式：
![[Pasted image 20250723184533.png]]

在评估结束后，采用$\epsilon$ -greedy 算法进行策略更新：
![[Pasted image 20250723184648.png]]

该算式表示，智能代理会以$\epsilon$ 的概率进行随机行动，否则选择贪婪行动。贪婪行动可以改进“策略”，随机行动则进行探索。

交替使用上述两个式子来评估和更新策略，之后就可以得到接近最优的策略。

#### 异策略型的SARSA

在异策略型的情况下，智能代理拥有两种策略，即行为策略和目标策略。智能代理首先会基于行为策略，通过采取各种行动收集大量样本数据， 然后再使用样本数据贪婪地更新目标策略。有两个要点：
- 行为策略和目标策略的概率分布相似则结果会稳定。对行为策略进行$\epsilon$-greedy更新，对目标策略进行贪婪更新。
- 使用重要性采样来矫正权重$\rho$。

重要性采样：
![[Pasted image 20250723185256.png]]

异策略型的SARSA更新式如下：
![[Pasted image 20250723185317.png]]

#### Q-Learning

重要性采样容易使结果变得不稳定，有较大方差。这会导致SARSA更新式中的目标跟着发生变化，从而使Q函数的更新变得不稳定。为了解决这个问题引入**Q-Learning。**

Q-Learning的三个特点：
- 采用TD方法
- 是异策略型
- 不使用重要性采样

![[Pasted image 20250723185615.png]]

贝尔曼最优方程：
![[Pasted image 20250723185732.png]]

贝尔曼最优方程使用了max算子。贝尔曼方程的回溯线形图如下：
![[Pasted image 20250723185823.png]]![[Pasted image 20250723191456.png]]

Q-Learning更新数学式如下：
![[Pasted image 20250723191447.png]]

## 6 DQN

DQN是基于Q学习和神经网络的算法。在此基础上，DQN引入了新的技术，即**“经验回放”和“目标网络”**。

#### 经验回放

当智能代理采取行动时会产生数据，比如在某个时刻得到的$E_t = (S_t, A_t, R_t, S_{t+1})$ ，其成为“经验数据”。经验回放就是将经验数据放入“缓冲区”中。然后，在更新Q函数时，从缓冲区中随机取出经验数据并使用。
![[Pasted image 20250723192207.png]]

通过经验回放，经验数据间的相关性会减弱，进而得到偏差小的数据。

#### 目标网络

在Q学习中，其会朝着TD目标更新Q函数，TD目标的值会随着Q函数的更新而变动，为了弥补这一差异，引入了固定TD目标的技术，这种技术称为“目标网络”。

首先，准备一个表示Q函数的原始网络（这个网络叫作qnet）。再准备了一个具有相同结构的网络（这个网络叫作qnet_target）。qnet通过正常的Q学习进行更新。qnet_target则定期与qnet的权重同步，在其余的时间里保持权重参数固定。接下来使用 qnet_target计算TD目标的值，就能抑制作为监督标签的TD目标的变动了。作为监督标签的TD目标不会一直变动，所以我们可以期待神经网络的训练能够保持稳定。


## 7 策略梯度法

前几章所有的方法都是基于价值的方法。除了基于价值的方法之外，还存在**不借助价值函数直接表示策略的方法，并用梯度来优化策略，该方法叫做策略梯度法(Policy Gradient Method)。**

策略梯度方法采用神经网络对策略进行建模。基于神经网络的策略表示为$\pi_{\theta}(a|s)$ 。

假定得到了以下“状态、行动、奖励“构成的时间序列数据。
![[Pasted image 20250723193406.png]]

上述序列数据称为**轨迹**。此时使用折现率可对**收益**作如下定义：
![[Pasted image 20250723193503.png]]

此时目标函数为：
![[Pasted image 20250723193524.png]]

对目标函数求梯度：
![[Pasted image 20250723193550.png]]

最后更新神经网络的参数：
![[Pasted image 20250723193617.png]]

注意到目标函数的梯度写成了期望的形式，通过蒙特卡洛方法就可以求得该期望值，进而求得梯度。令策略$\pi_{\theta}$的智能代理进行n回合行动，得到n个轨迹，通过对每个轨迹计算期望值内部式子，并求出平均值从而近似得到目标函数的梯度。
![[Pasted image 20250723210815.png]]

#### REINFORCE算法

在最基础的梯度策略方法中，权重$G(\tau)$包含时刻t之前的奖励，也就是说原本不相关的奖励作为噪声数据被包含在内了，为了去除噪声，对权重$G(\tau)$作如下修改：
![[Pasted image 20250723211233.png]]

权重$G_t$是在时刻t~T获得的奖励的总和。**基于上式的算法叫做REINFORCE**。

#### 基线(Baseline)

带基线的算法可以减小方差。

带基线的REINFORCE算法：
![[Pasted image 20250723211531.png]]

在该算式中$b(S_t)$是基线，基线可以是任何函数，可以是之前获得的奖励的平均值。


#### Actor-Critic

Actor-Critic是一个基于价值且基于策略的方法。

引入以下新符号：
- $\omega$ ：表示价值函数的神经网络的所有权重参数。
- $V_{\omega}(S_t)$：将价值函数模型化的神经网络。
此时目标函数的梯度如下：
![[Pasted image 20250723211924.png]]

将该基于蒙特卡洛方法的式子，改写成TD方法：

![[Pasted image 20250723212027.png]]

基于上式的算法就是Actor-Critic。我们需要同时训练**策略神经网络和价值函数神经网络**。


#### 总结策略梯度方法
![[Pasted image 20250723212158.png]]

## 8 其他深度强化学习算法

#### Proximal Policy Optimization（PPO）

PPO是一种基于梯度策略的方法，其受TRPO启发，通过约束模型更新参数的程度来进行策略优化。

目标函数如下：
![[Pasted image 20250723213910.png]]

![[Pasted image 20250723212935.png]]

在训练LLM的强化学习中，PPO要维护四个模型：
- 策略模型：用于给出action，也就是下一个token。
- 价值模型：用于计算优势函数。
- 奖励模型：计算reward。
- 参考模型：用于计算KL散度。
![[Pasted image 20250723213143.png]]

#### Direct Preference Optimization（DPO）

DPO隐式地表达了一种参数化奖励模型，让强化学习最大化目标函数的问题可以转换成对偏好对的二分类问题，进而通过简单的损失函数计算梯度优化参数。

目标函数如下：
![[Pasted image 20250723213443.png]]
![[Pasted image 20250723213457.png]]


#### Group Relative Policy Optimization（GRPO）

GRPO不显示维护一个Value Model，而是将一个query输入到Policy Model获得多个response，对于这些response计算与参考模型的KL散度，并将response输入金Reward Model得到多个reward，通过这些reward计算组相对优势。

目标函数如下：
![[Pasted image 20250723213857.png]]
![[Pasted image 20250723213927.png]]
![[Pasted image 20250723213938.png]]
